{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54f22484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a786049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14.0\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a084655c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.9.0\n",
      "  Downloading torchtext-0.9.0-cp39-cp39-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torchtext==0.9.0) (4.63.2)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torchtext==0.9.0) (2.28.1)\n",
      "Collecting torch==1.8.0\n",
      "  Downloading torch-1.8.0-cp39-cp39-manylinux1_x86_64.whl (735.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m735.5/735.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torchtext==0.9.0) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torch==1.8.0->torchtext==0.9.0) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests->torchtext==0.9.0) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests->torchtext==0.9.0) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests->torchtext==0.9.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests->torchtext==0.9.0) (2.1.1)\n",
      "Installing collected packages: torch, torchtext\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.13.0\n",
      "    Uninstalling torch-1.13.0:\n",
      "      Successfully uninstalled torch-1.13.0\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.14.0\n",
      "    Uninstalling torchtext-0.14.0:\n",
      "      Successfully uninstalled torchtext-0.14.0\n",
      "Successfully installed torch-1.8.0 torchtext-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "341a8a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65ee70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9.0\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "518c8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b6cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token.text for token in nlp(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3088ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Penn Treebank POS-tagging dataset\n",
    "dataset = torchtext.legacy.datasets.PennTreebank.splits(\n",
    "    text_field = torchtext.legacy.data.Field(tokenize = tokenize),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3cafa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[0]\n",
    "test_dataset = dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24fbf3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Dataset.__getattr__ at 0x7f4e064955f0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d144cd71",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Build the vocabulary\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[43mtorchtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torchtext/vocab.py:59\u001b[0m, in \u001b[0;36mVocab.__init__\u001b[0;34m(self, counter, max_size, min_freq, specials, vectors, unk_init, vectors_cache, specials_first)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m\"\"\"Create a Vocab object from a collections.Counter.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m        Default: True.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreqs \u001b[38;5;241m=\u001b[39m counter\n\u001b[0;32m---> 59\u001b[0m counter \u001b[38;5;241m=\u001b[39m \u001b[43mcounter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m()\n\u001b[1;32m     60\u001b[0m min_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(min_freq, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary\n",
    "vocab = torchtext.vocab.Vocab(train_dataset.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a96198c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fields for the dataset\n",
    "text_field = torchtext.legacy.data.Field(tokenize = tokenize)\n",
    "label_field = torchtext.legacy.data.Field(sequential = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96265207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-27 09:55:26--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
      "Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\n",
      "Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34869662 (33M) [application/x-gtar]\n",
      "Saving to: ‘simple-examples.tgz’\n",
      "\n",
      "simple-examples.tgz 100%[===================>]  33.25M  9.53MB/s    in 4.8s    \n",
      "\n",
      "2022-11-27 09:55:32 (6.86 MB/s) - ‘simple-examples.tgz’ saved [34869662/34869662]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz \n",
    "!tar xzf simple-examples.tgz -C dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44301cdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/dataset/ptb.train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset, test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtorchtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlegacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTabularDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath/to/dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mptb.train.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mptb.test.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_field\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_field\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torchtext/legacy/data/dataset.py:77\u001b[0m, in \u001b[0;36mDataset.splits\u001b[0;34m(cls, path, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdownload(root)\n\u001b[0;32m---> 77\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m val_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m validation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m     80\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, validation), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     81\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m     82\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, test), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torchtext/legacy/data/dataset.py:254\u001b[0m, in \u001b[0;36mTabularDataset.__init__\u001b[0;34m(self, path, format, fields, skip_header, csv_reader_params, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    250\u001b[0m make_example \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m: Example\u001b[38;5;241m.\u001b[39mfromJSON, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdict\u001b[39m\u001b[38;5;124m'\u001b[39m: Example\u001b[38;5;241m.\u001b[39mfromdict,\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtsv\u001b[39m\u001b[38;5;124m'\u001b[39m: Example\u001b[38;5;241m.\u001b[39mfromCSV, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m: Example\u001b[38;5;241m.\u001b[39mfromCSV}[\u001b[38;5;28mformat\u001b[39m]\n\u001b[0;32m--> 254\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpanduser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    256\u001b[0m         reader \u001b[38;5;241m=\u001b[39m unicode_csv_reader(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcsv_reader_params)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/dataset/ptb.train.txt'"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = torchtext.legacy.data.TabularDataset.splits(\n",
    "    path = \"./dataset/\",\n",
    "    train = \"ptb.train.txt\",\n",
    "    test = \"ptb.test.txt\",\n",
    "    format = \"tsv\",\n",
    "    fields = [(\"text\", text_field), (\"label\", label_field)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df3874ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from nltk) (4.63.2)\n",
      "Requirement already satisfied: click in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.10.31-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.0/770.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.7 regex-2022.10.31\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3c4def7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#uncomment if treebank not found\n",
    "nltk.download('treebank')\n",
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b16bd3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_data = treebank.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a4b6074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a35b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# Extract unique POS tags from training data\n",
    "all_tags = [tag for example in train_data for _, tag in example]\n",
    "tag_to_ix = defaultdict(lambda: len(tag_to_ix))\n",
    "for tag in all_tags:\n",
    "    tag_to_ix[tag]\n",
    "# Extract unique lemmas from training data\n",
    "all_wrd = [word for example in train_data for word, _ in example]\n",
    "word_to_ix = defaultdict(lambda: len(word_to_ix))\n",
    "for wrd in all_wrd:\n",
    "    word_to_ix[wrd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe6eb9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['NNP', 'NNP', ',', 'CD', 'NNS'], ['Pierre', 'Vinken', ',', '61', 'years'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tags[:5], all_wrd[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a805def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create POS tag and lemma lookup dictionaries\n",
    "tag_to_ix = {}\n",
    "ix_to_tag = {}\n",
    "word_to_ix = {}\n",
    "ix_to_word = {}\n",
    "for sentence in train_data:\n",
    "    for word, tag in sentence:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)\n",
    "            ix_to_tag[len(ix_to_tag)] = tag\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            ix_to_word[len(ix_to_word)] = word\n",
    "# Convert the training data to a list of feature/target pairs\n",
    "train_dataset = [([tag_to_ix[tag] for _, tag in sentence],\n",
    "               [word_to_ix[word] for word, _ in sentence])\n",
    "              for sentence in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d5f7391b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 1, 2, 3, 4, 1, 5, 6, 7, 8, 9, 7, 4, 8, 0, 2, 10],\n",
       " [0, 1, 2, 3, 4, 5, 2, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76edf8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa0c4354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class POS_Tagger(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # Shape: (num_words, batch_size)\n",
    "        embedded = self.embedding(text)\n",
    "        # Shape: (num_words, batch_size, embedding_dim)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        # Shape: (num_words, batch_size, vocab_size)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "50a78850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model and define the loss and optimizer\n",
    "model = POS_Tagger(len(all_wrd), 100, 100)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3850247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POS_Tagger(\n",
       "  (embedding): Embedding(100676, 100)\n",
       "  (rnn): LSTM(100, 100)\n",
       "  (linear): Linear(in_features=100, out_features=100676, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ef3d310",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable Example object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [45], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text, tags \u001b[38;5;129;01min\u001b[39;00m train_dataset:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m# Shape: (num_words, batch_size, vocab_size)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(text)\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# Shape: (num_words * batch_size, vocab_size)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable Example object"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    for text, tags in train_dataset:\n",
    "        # Shape: (num_words, batch_size, vocab_size)\n",
    "        output = model(text)\n",
    "        # Shape: (num_words * batch_size, vocab_size)\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        # Shape: (num_words * batch_size,)\n",
    "        tags = tags.view(-1)\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(output, tags)\n",
    "        # Perform backpropagation\n",
    "        loss.backward()\n",
    "        # Update the model parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5dd92966",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable Example object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [46], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text, tags \u001b[38;5;129;01min\u001b[39;00m test_dataset:\n\u001b[1;32m      6\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(text)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Shape: (num_words, batch_size, vocab_size)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable Example object"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for text, tags in test_dataset:\n",
    "    output = model(text)\n",
    "    # Shape: (num_words, batch_size, vocab_size)\n",
    "    _, predicted_tags = output.max(dim = -1)\n",
    "    # Shape: (num_words, batch_size)\n",
    "    correct += (predicted_tags == tags).sum().item()\n",
    "    total += tags.numel()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c8f33eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5b9ede04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    \"\"\"Fungsi untuk melakukan load data pada file .tsv maupun .txt\n",
    "    \n",
    "    File yang digunakan yaitu file .tsv maupun file yang mampu dibuka\n",
    "    menggunakan teks editor. Dalam file tersebut terdapat tag pembuka \n",
    "    dan diakhiri tag penutup  yang berfungsi sebagai penanda bahwa\n",
    "    entitas tersebut termasuk ke dalam satu kalimat.\n",
    "    \n",
    "    Args:\n",
    "        filename: string dari nama file yang akan diload datanya.\n",
    "        \n",
    "    Return:\n",
    "        list dari kata-kata dan tags dengan index yang menunjukkan posisi kalimat tersebut.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data dan buka sebagai file\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    with open(filename) as file:\n",
    "        contents = file.readlines()\n",
    "        \n",
    "    print(contents)\n",
    "\n",
    "    # Hapus karakter \\n yang tidak dibutuhkan\n",
    "    contents = [content.strip() for content in contents]\n",
    "    idx = 0\n",
    "    while idx < len(contents):\n",
    "        word = []\n",
    "        tag = []\n",
    "        # looping sampai menemukan pattern dengan awalan \n",
    "        while not contents[idx].startswith(\"'\"):\n",
    "            # kondisi jika menemukan sebuah data yang tidak memiliki awalan \n",
    "            if not contents[idx].startswith(\"'\"):\n",
    "                temp_word, temp_tag = contents[idx].split(\"\\t\")\n",
    "                word.append(temp_word.lower())\n",
    "                tag.append(temp_tag)\n",
    "            idx += 1\n",
    "        sentences.append(word)\n",
    "        tags.append(tag)\n",
    "        idx += 2\n",
    "        \n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dd90184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(multi_list):\n",
    "    \"\"\"Mengembalikan list multi dimensi ke dalam list satu dimensi.\n",
    "    \n",
    "    Input list yang masuk di proses menggunakan method from_iterable dari\n",
    "    package itertools dan mengembalikan object berupa generator yang iterable.\n",
    "    \n",
    "    Args:\n",
    "        multi_list: list multi dimensi.\n",
    "    \n",
    "    Return:\n",
    "        list satu dimensi yang nantinya digunakan untuk pemrosesan lebih lanjut.\n",
    "    \"\"\"\n",
    "    \n",
    "    return chain.from_iterable(multi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "feec950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emission_table(words, tags):\n",
    "    \"\"\"Membuat tabel emisi dari input kata dan tags.\n",
    "    \n",
    "    Fungsi untuk mengembalikan sebuah tabel dari kata dan kemunculan tags terhadap kata tersebut.\n",
    "    \n",
    "    Args:\n",
    "        words: list kata.\n",
    "        tags: list tags.\n",
    "    \n",
    "    Return:\n",
    "        dictionary yang merepresntasikan tabel emisi.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Membuat representasi tabel emission probability dari HMM\n",
    "    hidden_state = {}\n",
    "    for word in words:\n",
    "        # Jika word belum pernah ditemui maka akan digenerate\n",
    "        # tagset berserta probabilitasnya\n",
    "        if word not in hidden_state.keys():\n",
    "            word_tags = []\n",
    "            for idx, wrd in enumerate(words):\n",
    "                if wrd == word :\n",
    "                    word_tags.append(tags[idx])\n",
    "            # Membuat dictionary berisi semua tagset dari sebuah word\n",
    "            # pada korpus. Berserta jumlah kemunculan setiap tagsetnya\n",
    "            tag_count = {}\n",
    "            for tag in word_tags:\n",
    "                if tag not in tag_count.keys():\n",
    "                    tag_count[tag] = 1\n",
    "                else:\n",
    "                    tag_count[tag] += 1\n",
    "            total = 0\n",
    "            # Mengubah jumlah kemunculan menjadi probabilitas\n",
    "            tag_prob = {}\n",
    "            for count in tag_count.values():\n",
    "                total += count\n",
    "            for tagset in tag_count.keys():\n",
    "                tag_prob[tagset] = tag_count[tagset]/total\n",
    "            hidden_state[word] = tag_prob\n",
    "        else:\n",
    "            # Jika word sudah pernah ditemui maka akan dilewati\n",
    "            continue\n",
    "    return(hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9ed29e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [69], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dataset/corpus.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m sentences, tags \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [66], line 34\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m contents[idx]\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# kondisi jika menemukan sebuah data yang tidak memiliki awalan \u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m contents[idx]\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 34\u001b[0m         temp_word, temp_tag \u001b[38;5;241m=\u001b[39m contents[idx]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m         word\u001b[38;5;241m.\u001b[39mappend(temp_word\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m     36\u001b[0m         tag\u001b[38;5;241m.\u001b[39mappend(temp_tag)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "filename = './dataset/corpus.tsv'\n",
    "sentences, tags = get_data(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b6ba35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4be5f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m groupby\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List, Callable, Dict\n",
    "from collections import OrderedDict\n",
    "from itertools import groupby\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4a1759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
