{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23724888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps'), ('jumps', 'over'), ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# tokenize the text\n",
    "tokens = nltk.word_tokenize(\"The quick brown fox jumps over the lazy dog\")\n",
    "\n",
    "# generate bigrams\n",
    "bigrams = nltk.ngrams(tokens, 2)\n",
    "\n",
    "# print the bigrams\n",
    "print(list(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c2ba8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "brown\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "# define the input text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# tokenize the text and convert the tokens to lowercase\n",
    "tokens = [w.lower() for w in nltk.word_tokenize(text)]\n",
    "\n",
    "# generate the trigrams\n",
    "trigrams = ngrams(tokens, 3)\n",
    "\n",
    "# compute the conditional frequency distribution of the next word given the previous two words\n",
    "cfd = ConditionalFreqDist([((w1, w2), w3) for w1, w2, w3 in trigrams])\n",
    "\n",
    "# print the raw frequency of the trigram \"the quick brown\"\n",
    "print(cfd[('the', 'quick')]['brown'])\n",
    "\n",
    "# predict the next word in the sequence \"the quick\"\n",
    "next_word_dist = cfd[('the', 'quick')]\n",
    "\n",
    "# print the predicted next word and its probability\n",
    "print(next_word_dist.max())\n",
    "print(next_word_dist.freq(next_word_dist.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd550851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps'), ('jumps', 'over'), ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog')]\n"
     ]
    }
   ],
   "source": [
    "import torchtext.legacy.data\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "\n",
    "# define the input text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# define the tokenizer\n",
    "tokenizer = lambda x: x.split()\n",
    "\n",
    "# define the field that will be used to tokenize the text\n",
    "field = torchtext.legacy.data.Field(sequential=True, tokenize=tokenizer)\n",
    "\n",
    "# tokenize the text\n",
    "tokens = field.tokenize(text)\n",
    "\n",
    "# create the bigrams from the tokenized text\n",
    "bigrams = list(ngrams_iterator(tokens, 2))[len(tokens):]\n",
    "\n",
    "# convert the bigrams to a list of tuples\n",
    "bigrams = [tuple(bigram.split()) for bigram in bigrams]\n",
    "\n",
    "# print the bigrams\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "026ff053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps'), ('jumps', 'over'), ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog')]\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "import torchtext.legacy.data\n",
    "\n",
    "# define the input text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# define the tokenizer\n",
    "tokenizer = lambda x: x.split()\n",
    "\n",
    "# define the field that will be used to tokenize the text\n",
    "field = torchtext.legacy.data.Field(sequential=True, tokenize=tokenizer, lower=False)\n",
    "\n",
    "# tokenize the text\n",
    "tokens = field.tokenize(text)\n",
    "\n",
    "# create the bigrams from the tokenized text\n",
    "bigrams = list(torchtext.data.utils.ngrams_iterator(tokens, 2))[len(tokens):]\n",
    "\n",
    "# convert the bigrams to a list of tuples\n",
    "bigrams = [tuple(bigram.split()) for bigram in bigrams]\n",
    "\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3979641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_key(example):\n",
    "    # return the length of the first bigram in the example\n",
    "    return len(example[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c94ff594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps'), ('jumps', 'over'), ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog')]\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "\n",
    "# define the input text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# define the tokenizer\n",
    "tokenizer = lambda x: x.split()\n",
    "\n",
    "# tokenize the text\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "# create the bigrams from the tokenized text\n",
    "bigrams = list(torchtext.data.utils.ngrams_iterator(tokens, 2))[len(tokens):]\n",
    "\n",
    "# convert the bigrams to a list of tuples\n",
    "bigrams = [tuple(bigram.split()) for bigram in bigrams]\n",
    "\n",
    "print(bigrams)\n",
    "# define a field for the bigrams\n",
    "bigram_field = torchtext.legacy.data.Field(sequential=True, tokenize=tokenizer, use_vocab=True)\n",
    "\n",
    "\n",
    "# define an example for each bigram\n",
    "examples = [torchtext.legacy.data.Example.fromlist([bigram], [('bigram', bigram_field)]) for bigram in bigrams]\n",
    "\n",
    "# define a dataset from the examples and the bigram field\n",
    "dataset = torchtext.legacy.data.Dataset(examples, [('bigram', bigram_field)])\n",
    "\n",
    "# define an iterator for the dataset\n",
    "iterator = torchtext.legacy.data.BucketIterator(dataset, batch_size=2, train=False, sort=False)\n",
    "\n",
    "# print the bigrams in the first batch\n",
    "# print('bigrams in the first batch:', next(iter(iterator)).bigram)\n",
    "\n",
    "# for example in next(iter(iterator)):\n",
    "#     print('bigrams in the first batch:', example.bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cf7397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_sequence():\n",
    "    sequence = [\"I\", \"had\", \"a\", \"sandwich\", \"for\", \"lunch\", \"yesterday\", \"and\", \"it\", \"was\", \"delicious\"]\n",
    "    return sequence\n",
    "\n",
    "class NGramModel(torch.nn.Module):\n",
    "    def __init__(self, num_words, num_hidden, n=2):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.embedding = torch.nn.Embedding(num_words, num_hidden)\n",
    "        self.linear = torch.nn.Linear(num_hidden * n, num_words)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05949dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, sequence, word_to_idx, learning_rate=1e-2, epochs=100):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "\n",
    "        # Loop through the words in the sequence\n",
    "        for i in range(len(sequence) - model.n):\n",
    "            # Convert the current n-gram and next word to tensors\n",
    "            current_ngram = torch.tensor([word_to_idx[word] for word in sequence[i:i+model.n]], dtype=torch.long)\n",
    "            next_word = torch.tensor([word_to_idx[sequence[i + model.n]]], dtype=torch.long)\n",
    "\n",
    "            # Predict the probability of the next word\n",
    "            logits = model(current_ngram.unsqueeze(0))\n",
    "\n",
    "            # Compute the cross-entropy loss and update the totals\n",
    "            loss = criterion(logits, next_word)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute the accuracy and update the totals\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            total_accuracy += (predicted == next_word).sum().item()\n",
    "\n",
    "            # Backpropagate the loss and update the model parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Compute the average loss and accuracy for the epoch and update the relevant lists\n",
    "        avg_loss = total_loss / (len(sequence) - model.n)\n",
    "        avg_accuracy = total_accuracy / (len(sequence) - model.n)\n",
    "        losses.append(avg_loss)\n",
    "        accuracies.append(avg_accuracy)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: loss = {avg_loss:.4f}, accuracy = {avg_accuracy:.4f}\")\n",
    "\n",
    "    return model, losses, accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb93286e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: loss = 0.0569, accuracy = 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Generate the training data\n",
    "sequence = generate_sequence()\n",
    "# Create a vocabulary of unique words in the sequence\n",
    "vocab = list(set(sequence))\n",
    "\n",
    "# Create a mapping from word to index\n",
    "word_to_idx = {word: index for index, word in enumerate(vocab)}\n",
    "\n",
    "# Create a mapping from index to word\n",
    "index_to_word = {index: word for index, word in enumerate(vocab)}\n",
    "\n",
    "# Define the number of words, the number of hidden units, and the value of n\n",
    "num_words = len(set(sequence))\n",
    "num_hidden = 10\n",
    "n = 3\n",
    "\n",
    "# Initialize the n-gram model\n",
    "model = NGramModel(num_words, num_hidden, n)\n",
    "\n",
    "# Train the model\n",
    "model, losses, accuracies = train_model(model,sequence,word_to_idx,1e-2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0927fafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sandwich': 0,\n",
       " 'was': 1,\n",
       " 'yesterday': 2,\n",
       " 'lunch': 3,\n",
       " 'I': 4,\n",
       " 'and': 5,\n",
       " 'it': 6,\n",
       " 'had': 7,\n",
       " 'a': 8,\n",
       " 'for': 9,\n",
       " 'delicious': 10}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a952dad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'sandwich',\n",
       " 1: 'was',\n",
       " 2: 'yesterday',\n",
       " 3: 'lunch',\n",
       " 4: 'I',\n",
       " 5: 'and',\n",
       " 6: 'it',\n",
       " 7: 'had',\n",
       " 8: 'a',\n",
       " 9: 'for',\n",
       " 10: 'delicious'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a748694",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_ngram = torch.tensor([word_to_idx['had'],word_to_idx['a'],word_to_idx[\"sandwich\"]], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c7000c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_logits = model(_ngram.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45693815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[torch.max(_logits, dim=1)[1].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309b827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
