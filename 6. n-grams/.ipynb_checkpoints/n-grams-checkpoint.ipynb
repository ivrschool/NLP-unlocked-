{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23724888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps'), ('jumps', 'over'), ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# tokenize the text\n",
    "tokens = nltk.word_tokenize(\"The quick brown fox jumps over the lazy dog\")\n",
    "\n",
    "# generate bigrams\n",
    "bigrams = nltk.ngrams(tokens, 2)\n",
    "\n",
    "# print the bigrams\n",
    "print(list(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f9f6e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'normalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m next_word_dist \u001b[38;5;241m=\u001b[39m freq_dist[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquick\u001b[39m\u001b[38;5;124m'\u001b[39m)]\u001b[38;5;66;03m#.copy()\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# normalize the probabilities\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mnext_word_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# print the predicted next word and its probability\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(next_word_dist\u001b[38;5;241m.\u001b[39mmax())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'normalize'"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# define the input text\n",
    "text = \"the quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# generate the bigrams\n",
    "bigrams = ngrams(tokens, 2)\n",
    "\n",
    "# compute the raw frequency of each bigram\n",
    "freq_dist = FreqDist(bigrams)\n",
    "\n",
    "# print the raw frequency of the bigram \"the quick\"\n",
    "print(freq_dist[('the', 'quick')])\n",
    "\n",
    "# predict the next word in the sequence \"the quick\"\n",
    "next_word_dist = freq_dist[('the', 'quick')]#.copy()\n",
    "\n",
    "# normalize the probabilities\n",
    "next_word_dist.normalize()\n",
    "\n",
    "# print the predicted next word and its probability\n",
    "print(next_word_dist.max())\n",
    "print(next_word_dist[next_word_dist.max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cecb024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A FreqDist must have at least one sample before max is defined.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m next_word_dist \u001b[38;5;241m=\u001b[39m cfd[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquick\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# print the predicted next word and its probability\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnext_word_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(next_word_dist\u001b[38;5;241m.\u001b[39mfreq(next_word_dist\u001b[38;5;241m.\u001b[39mmax()))\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/nltk/probability.py:242\u001b[0m, in \u001b[0;36mFreqDist.max\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03mReturn the sample with the greatest number of outcomes in this\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03mfrequency distribution.  If two or more samples have the same\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m:rtype: any or None\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA FreqDist must have at least one sample before max is defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: A FreqDist must have at least one sample before max is defined."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "# define the input text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# generate the trigrams\n",
    "trigrams = ngrams(tokens, 3)\n",
    "\n",
    "# compute the conditional frequency distribution of the next word given the previous two words\n",
    "# cfd = ConditionalFreqDist([(w1_w2, w3) for w1_w2, w3 in trigrams])\n",
    "cfd = ConditionalFreqDist([((w1, w2), w3) for w1, w2, w3 in trigrams])\n",
    "\n",
    "\n",
    "# print the raw frequency of the trigram \"the quick brown\"\n",
    "print(cfd[('the', 'quick')]['brown'])\n",
    "\n",
    "# predict the next word in the sequence \"the quick\"\n",
    "next_word_dist = cfd[('the', 'quick')]\n",
    "\n",
    "# print the predicted next word and its probability\n",
    "print(next_word_dist.max())\n",
    "print(next_word_dist.freq(next_word_dist.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3819a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "brown\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "# define the input text\n",
    "text = \"the quick brown fox jumps over the lazy dog\"\n",
    "text = text.lower()\n",
    "# tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# generate the trigrams\n",
    "trigrams = ngrams(tokens, 3)\n",
    "\n",
    "# compute the conditional frequency distribution of the next word given the previous two words\n",
    "# cfd = ConditionalFreqDist([(w1_w2, w3) for w1_w2, w3 in trigrams])\n",
    "cfd = ConditionalFreqDist([((w1, w2), w3) for w1, w2, w3 in trigrams])\n",
    "\n",
    "# print the raw frequency of the trigram \"the quick brown\"\n",
    "print(cfd[('the', 'quick')]['brown'])\n",
    "\n",
    "# predict the next word in the sequence \"the quick\"\n",
    "next_word_dist = cfd[('the', 'quick')]\n",
    "\n",
    "# print the predicted next word and its probability\n",
    "print(next_word_dist.max())\n",
    "print(next_word_dist.freq(next_word_dist.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c2ba8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "brown\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "# define the input text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# tokenize the text and convert the tokens to lowercase\n",
    "tokens = [w.lower() for w in nltk.word_tokenize(text)]\n",
    "\n",
    "# generate the trigrams\n",
    "trigrams = ngrams(tokens, 3)\n",
    "\n",
    "# compute the conditional frequency distribution of the next word given the previous two words\n",
    "cfd = ConditionalFreqDist([((w1, w2), w3) for w1, w2, w3 in trigrams])\n",
    "\n",
    "# print the raw frequency of the trigram \"the quick brown\"\n",
    "print(cfd[('the', 'quick')]['brown'])\n",
    "\n",
    "# predict the next word in the sequence \"the quick\"\n",
    "next_word_dist = cfd[('the', 'quick')]\n",
    "\n",
    "# print the predicted next word and its probability\n",
    "print(next_word_dist.max())\n",
    "print(next_word_dist.freq(next_word_dist.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832221f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d502db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fe3dc6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'ngrams'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# define the n-gram field\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m field \u001b[38;5;241m=\u001b[39m \u001b[43mtorchtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlegacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngrams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# apply the field to the text to produce the bigrams\u001b[39;00m\n\u001b[1;32m     15\u001b[0m bigrams \u001b[38;5;241m=\u001b[39m [field\u001b[38;5;241m.\u001b[39mprocess([text])]\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'ngrams'"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "# define the input text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# tokenize the text\n",
    "tokenizer = lambda x: x.split()\n",
    "\n",
    "# define the n-gram field\n",
    "field = torchtext.legacy.data.Field(sequential=True, tokenize=tokenizer, ngrams=2)\n",
    "\n",
    "# apply the field to the text to produce the bigrams\n",
    "bigrams = [field.process([text])]\n",
    "\n",
    "# define the iterator\n",
    "iterator = torchtext.legacy.data.BucketIterator(bigrams, batch_size=2, train=False)\n",
    "\n",
    "# print the bigrams in the first batch\n",
    "print(next(iter(iterator))[0].text)\n",
    "\n",
    "# define the neural network\n",
    "class NGramLM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLM, self).__init__()\n",
    "        self.embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = torch.nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "# define the model parameters\n",
    "vocab_size = len(field.vocab)\n",
    "embedding_dim = 50\n",
    "context_size = 2\n",
    "\n",
    "# initialize the model\n",
    "model = NGramLM(vocab_size, embedding_dim, context_size)\n",
    "\n",
    "# print the model architecture\n",
    "print(model)\n",
    "\n",
    "# define the loss function\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# define the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in iterator:\n",
    "        # zero the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        log_probs = model(batch.text)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = loss_function(log_probs, batch.target)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # print the average loss per epoch\n",
    "    print(f\"Epoch: {epoch+1} \\t Loss: {total_loss/len(iterator)}\")\n",
    "\n",
    "# predict the next word given the sequence \"the quick\"\n",
    "with torch.no_grad():\n",
    "    inputs = field.process([\"the quick\"])\n",
    "    log_probs = model(inputs)\n",
    "    probs = torch.exp(log_probs)\n",
    "\n",
    "# print the predicted next word and its probability\n",
    "print(field.vocab.itos[torch.argmax(probs)])\n",
    "print(torch.max(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf9f139f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'ngrams'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# define the n-gram field\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m field \u001b[38;5;241m=\u001b[39m \u001b[43mtorchtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlegacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngrams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# apply the field to the text to produce the bigrams\u001b[39;00m\n\u001b[1;32m     15\u001b[0m bigrams \u001b[38;5;241m=\u001b[39m [field\u001b[38;5;241m.\u001b[39mprocess([text])]\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'ngrams'"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "# define the input text\n",
    "text = \"the quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# tokenize the text\n",
    "tokenizer = lambda x: x.split()\n",
    "\n",
    "# define the n-gram field\n",
    "field = torchtext.legacy.data.Field(sequential=True, tokenize=tokenizer, ngrams=2)\n",
    "\n",
    "# apply the field to the text to produce the bigrams\n",
    "bigrams = [field.process([text])]\n",
    "\n",
    "# apply the Ngrams function to the tokens to produce the bigrams\n",
    "bigrams = torchtext.data.Ngrams(2)(tokens)\n",
    "\n",
    "# bigrams = torchtext.data.Ngrams(2)(tokenizer(text))\n",
    "\n",
    "# print(bigrams)\n",
    "\n",
    "# define the iterator\n",
    "iterator = torchtext.legacy.data.BucketIterator(bigrams, batch_size=2, train=False)\n",
    "\n",
    "# print the bigrams in the first batch\n",
    "print(next(iter(iterator))[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fde29079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'the quick', 'quick brown', 'brown fox', 'fox jumps', 'jumps over', 'over the', 'the lazy', 'lazy dog']\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer, ngrams_iterator\n",
    "\n",
    "# define the input text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# define the tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# tokenize the text\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "# generate the bigrams\n",
    "bigrams = list(ngrams_iterator(tokens, 2))\n",
    "\n",
    "print(bigrams)\n",
    "\n",
    "# # define the iterator\n",
    "# iterator = torchtext.legacy.data.BucketIterator(bigrams, batch_size=2, train=False)\n",
    "\n",
    "# # print the bigrams in the first batch\n",
    "# print(next(iter(iterator))[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fd550851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps'), ('jumps', 'over'), ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog')]\n"
     ]
    }
   ],
   "source": [
    "import torchtext.legacy.data\n",
    "\n",
    "# define the input text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# define the tokenizer\n",
    "tokenizer = lambda x: x.split()\n",
    "\n",
    "# define the field that will be used to tokenize the text\n",
    "field = torchtext.legacy.data.Field(sequential=True, tokenize=tokenizer)\n",
    "\n",
    "# tokenize the text\n",
    "tokens = field.tokenize(text)\n",
    "\n",
    "# create the bigrams from the tokenized text\n",
    "bigrams = list(ngrams_iterator(tokens, 2))[len(tokens):]\n",
    "\n",
    "# convert the bigrams to a list of tuples\n",
    "bigrams = [tuple(bigram.split()) for bigram in bigrams]\n",
    "\n",
    "# print the bigrams\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d6ef97e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'sort_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# define the iterator\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[43mtorchtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlegacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBucketIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbigrams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# print the bigrams in the first batch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(iterator))[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torchtext/legacy/data/iterator.py:59\u001b[0m, in \u001b[0;36mIterator.__init__\u001b[0;34m(self, dataset, batch_size, sort_key, device, batch_size_fn, train, repeat, shuffle, sort, sort_within_batch)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_within_batch \u001b[38;5;241m=\u001b[39m sort_within_batch\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sort_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_key \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_key\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_key \u001b[38;5;241m=\u001b[39m sort_key\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'sort_key'"
     ]
    }
   ],
   "source": [
    "# define the iterator\n",
    "iterator = torchtext.legacy.data.BucketIterator(bigrams, batch_size=2, train=False)\n",
    "\n",
    "# print the bigrams in the first batch\n",
    "print(next(iter(iterator))[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "026ff053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps'), ('jumps', 'over'), ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog')]\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "import torchtext.legacy.data\n",
    "\n",
    "# define the input text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# define the tokenizer\n",
    "tokenizer = lambda x: x.split()\n",
    "\n",
    "# define the field that will be used to tokenize the text\n",
    "field = torchtext.legacy.data.Field(sequential=True, tokenize=tokenizer, lower=False)\n",
    "\n",
    "# tokenize the text\n",
    "tokens = field.tokenize(text)\n",
    "\n",
    "# create the bigrams from the tokenized text\n",
    "bigrams = list(torchtext.data.utils.ngrams_iterator(tokens, 2))[len(tokens):]\n",
    "\n",
    "# convert the bigrams to a list of tuples\n",
    "bigrams = [tuple(bigram.split()) for bigram in bigrams]\n",
    "\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "14099d0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'Example' and 'Example'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [84], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m iterator \u001b[38;5;241m=\u001b[39m torchtext\u001b[38;5;241m.\u001b[39mlegacy\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mBucketIterator(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# print the bigrams in the first batch\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbigram)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torchtext/legacy/data/iterator.py:145\u001b[0m, in \u001b[0;36mIterator.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, minibatch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches):\n\u001b[1;32m    147\u001b[0m             \u001b[38;5;66;03m# fast-forward if loaded from state\u001b[39;00m\n\u001b[1;32m    148\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterations_this_epoch \u001b[38;5;241m>\u001b[39m idx:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torchtext/legacy/data/iterator.py:121\u001b[0m, in \u001b[0;36mIterator.init_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_state_this_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_shuffler\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restored_from_state:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restored_from_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torchtext/legacy/data/iterator.py:249\u001b[0m, in \u001b[0;36mBucketIterator.create_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort:\n\u001b[0;32m--> 249\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches \u001b[38;5;241m=\u001b[39m batch(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m    250\u001b[0m                              \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size_fn)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches \u001b[38;5;241m=\u001b[39m pool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m    253\u001b[0m                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size_fn,\n\u001b[1;32m    254\u001b[0m                             random_shuffler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_shuffler,\n\u001b[1;32m    255\u001b[0m                             shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshuffle,\n\u001b[1;32m    256\u001b[0m                             sort_within_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_within_batch)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torchtext/legacy/data/iterator.py:106\u001b[0m, in \u001b[0;36mIterator.data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m\"\"\"Return the examples in the dataset in order, sorted, or shuffled.\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort:\n\u001b[0;32m--> 106\u001b[0m     xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshuffle:\n\u001b[1;32m    108\u001b[0m     xs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_shuffler(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset)))]\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'Example' and 'Example'"
     ]
    }
   ],
   "source": [
    "# import the necessary classes from the torchtext.legacy.data module\n",
    "from torchtext.legacy.data import Dataset, Example, Field\n",
    "\n",
    "# # create the fields that the dataset will use\n",
    "fields = [('bigram', Field(sequential=True))]\n",
    "\n",
    "# create a list of examples from the bigrams\n",
    "examples = [Example.fromlist([bigram], fields) for bigram in bigrams]\n",
    "\n",
    "# create the dataset from the examples\n",
    "dataset = Dataset(examples, fields)\n",
    "\n",
    "# create the iterator from the dataset\n",
    "iterator = torchtext.legacy.data.BucketIterator(dataset, batch_size=2, train=False)\n",
    "\n",
    "# print the bigrams in the first batch\n",
    "print(next(iter(iterator)).bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f3979641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_key(example):\n",
    "    # return the length of the first bigram in the example\n",
    "    return len(example[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c94ff594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps'), ('jumps', 'over'), ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog')]\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "\n",
    "# define the input text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# define the tokenizer\n",
    "tokenizer = lambda x: x.split()\n",
    "\n",
    "# tokenize the text\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "# create the bigrams from the tokenized text\n",
    "bigrams = list(torchtext.data.utils.ngrams_iterator(tokens, 2))[len(tokens):]\n",
    "\n",
    "# convert the bigrams to a list of tuples\n",
    "bigrams = [tuple(bigram.split()) for bigram in bigrams]\n",
    "\n",
    "print(bigrams)\n",
    "# define a field for the bigrams\n",
    "bigram_field = torchtext.legacy.data.Field(sequential=True, tokenize=tokenizer, use_vocab=True)\n",
    "\n",
    "\n",
    "# define an example for each bigram\n",
    "examples = [torchtext.legacy.data.Example.fromlist([bigram], [('bigram', bigram_field)]) for bigram in bigrams]\n",
    "\n",
    "# define a dataset from the examples and the bigram field\n",
    "dataset = torchtext.legacy.data.Dataset(examples, [('bigram', bigram_field)])\n",
    "\n",
    "# define an iterator for the dataset\n",
    "iterator = torchtext.legacy.data.BucketIterator(dataset, batch_size=2, train=False, sort=False)\n",
    "\n",
    "# print the bigrams in the first batch\n",
    "# print('bigrams in the first batch:', next(iter(iterator)).bigram)\n",
    "\n",
    "# for example in next(iter(iterator)):\n",
    "#     print('bigrams in the first batch:', example.bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5714c284",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Field' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [100], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbigrams in the first batch:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbigram)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torchtext/legacy/data/iterator.py:160\u001b[0m, in \u001b[0;36mIterator.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m             minibatch\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_key, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminibatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepeat:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torchtext/legacy/data/batch.py:34\u001b[0m, in \u001b[0;36mBatch.__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mgetattr\u001b[39m(x, name) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, \u001b[43mfield\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torchtext/legacy/data/field.py:231\u001b[0m, in \u001b[0;36mField.process\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m\"\"\" Process a list of examples to create a torch.Tensor.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03mPad, numericalize, and postprocess a batch and create a tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    and custom postprocessing Pipeline.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    230\u001b[0m padded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad(batch)\n\u001b[0;32m--> 231\u001b[0m tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumericalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torchtext/legacy/data/field.py:330\u001b[0m, in \u001b[0;36mField.numericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_vocab:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequential:\n\u001b[0;32m--> 330\u001b[0m         arr \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mstoi[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ex] \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m arr]\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m         arr \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mstoi[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arr]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torchtext/legacy/data/field.py:330\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_vocab:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequential:\n\u001b[0;32m--> 330\u001b[0m         arr \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mstoi[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ex] \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m arr]\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m         arr \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mstoi[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arr]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torchtext/legacy/data/field.py:330\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_vocab:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequential:\n\u001b[0;32m--> 330\u001b[0m         arr \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241m.\u001b[39mstoi[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ex] \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m arr]\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m         arr \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mstoi[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arr]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Field' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "print('bigrams in the first batch:', next(iter(iterator)).bigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the neural network\n",
    "class NGramLM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLM, self).__init__()\n",
    "        self.embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = torch.nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "# define the model parameters\n",
    "vocab_size = len(field.vocab)\n",
    "embedding_dim = 50\n",
    "context_size = 2\n",
    "\n",
    "# initialize the model\n",
    "model = NGramLM(vocab_size, embedding_dim, context_size)\n",
    "\n",
    "# print the model architecture\n",
    "print(model)\n",
    "\n",
    "# define the loss function\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# define the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in iterator:\n",
    "        # zero the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        log_probs = model(batch.text)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = loss_function(log_probs, batch.target)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # print the average loss per epoch\n",
    "    print(f\"Epoch: {epoch+1} \\t Loss: {total_loss/len(iterator)}\")\n",
    "\n",
    "# predict the next word given the sequence \"the quick\"\n",
    "with torch.no_grad():\n",
    "    inputs = field.process([\"the quick\"])\n",
    "    log_probs = model(inputs)\n",
    "    probs = torch.exp(log_probs)\n",
    "\n",
    "# print the predicted next word and its probability\n",
    "print(field.vocab.itos[torch.argmax(probs)])\n",
    "print(torch.max(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c36ab6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchtext.legacy' has no attribute 'transforms'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [33], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe quick brown fox jumps over the lazy dog\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# create a tokenizer\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorchtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlegacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241m.\u001b[39mBasicEnglishNormalize()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# tokenize the text\u001b[39;00m\n\u001b[1;32m     10\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer(text)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchtext.legacy' has no attribute 'transforms'"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "\n",
    "# define the input text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# create a tokenizer\n",
    "tokenizer = torchtext.legacy.transforms.BasicEnglishNormalize()\n",
    "\n",
    "# tokenize the text\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "# create a bigram transform\n",
    "bigram_transform = torchtext.transforms.NGramTransform(n=2)\n",
    "\n",
    "# apply the bigram transform to the tokens to produce the bigrams\n",
    "bigrams = bigram_transform(tokens)\n",
    "\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3065fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_sequence():\n",
    "    sequence = [\"I\", \"had\", \"a\", \"sandwich\", \"for\", \"lunch\", \"yesterday\", \"and\", \"it\", \"was\", \"delicious\"]\n",
    "    return sequence\n",
    "\n",
    "class NGramModel(torch.nn.Module):\n",
    "    def __init__(self, num_words, num_hidden, n=2):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.embedding = torch.nn.Embedding(num_words, num_hidden)\n",
    "        self.linear = torch.nn.Linear(num_hidden * n, num_words)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8adb96e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, sequence, learning_rate=1e-2, epochs=100):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "\n",
    "        # Convert the words in the sequence to indices\n",
    "        idx_sequence = [word_to_idx[word] for word in sequence]\n",
    "\n",
    "        # Loop through the words in the sequence\n",
    "        for i in range(model.n, len(sequence)):\n",
    "            # Convert the current and next words to tensors\n",
    "            context = torch.tensor(idx_sequence[i - model.n:i], dtype=torch.long).unsqueeze(0)\n",
    "            next_word = torch.tensor([idx_sequence[i]], dtype=torch.long)\n",
    "\n",
    "            # Predict the probability of the next word\n",
    "            logits = model(context)\n",
    "\n",
    "            # Compute the cross-entropy loss and update the totals\n",
    "            loss = criterion(logits, next_word)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute the accuracy and update the totals\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            total_accuracy += (predicted == next_word).sum().item()\n",
    "\n",
    "            # Backpropagate the loss and update the model parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Compute the average loss and accuracy for the epoch .and update the relevant lists\n",
    "        avg_loss = total_loss / (len(sequence) - model.n)\n",
    "        avg_accuracy = total_accuracy / (len(sequence) - model.n)\n",
    "        losses.append(avg_loss)\n",
    "        accuracies.append(avg_accuracy)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: loss = {avg_loss:.4f}, accuracy = {avg_accuracy:.4f}\")\n",
    "\n",
    "    return model, losses, accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee3a50c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'idx_sequence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43midx_sequence\u001b[49m[i \u001b[38;5;241m-\u001b[39m model\u001b[38;5;241m.\u001b[39mn:i], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m next_word \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([idx_sequence[i]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m      3\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(context)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'idx_sequence' is not defined"
     ]
    }
   ],
   "source": [
    "context = torch.tensor(idx_sequence[i - model.n:i], dtype=torch.long).unsqueeze(0)\n",
    "next_word = torch.tensor([idx_sequence[i]], dtype=torch.long)\n",
    "logits = model(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf5c90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel(torch.nn.Module):\n",
    "    def __init__(self, num_words, num_hidden, n):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(num_words, num_hidden)\n",
    "        self.linear = torch.nn.Linear(num_hidden * n, num_words)\n",
    "        self.n = n\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(-1, self.n * x.shape[2])\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aabadb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, sequence, word_to_idx, learning_rate=1e-2, epochs=100):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "\n",
    "        # Loop through the words in the sequence\n",
    "        for i in range(len(sequence) - model.n):\n",
    "            # Convert the current n-gram and next word to tensors\n",
    "            current_ngram = torch.tensor([word_to_idx[word] for word in sequence[i:i+model.n]], dtype=torch.long)\n",
    "            next_word = torch.tensor([word_to_idx[sequence[i + model.n]]], dtype=torch.long)\n",
    "\n",
    "            # Predict the probability of the next word\n",
    "            logits = model(current_ngram.unsqueeze(0))\n",
    "\n",
    "            # Compute the cross-entropy loss and update the totals\n",
    "            loss = criterion(logits, next_word)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute the accuracy and update the totals\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            total_accuracy += (predicted == next_word).sum().item()\n",
    "\n",
    "            # Backpropagate the loss and update the model parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Compute the average loss and accuracy for the epoch and update the relevant lists\n",
    "        avg_loss = total_loss / (len(sequence) - model.n)\n",
    "        avg_accuracy = total_accuracy / (len(sequence) - model.n)\n",
    "        losses.append(avg_loss)\n",
    "        accuracies.append(avg_accuracy)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: loss = {avg_loss:.4f}, accuracy = {avg_accuracy:.4f}\")\n",
    "\n",
    "    return model, losses, accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cf7397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_sequence():\n",
    "    sequence = [\"I\", \"had\", \"a\", \"sandwich\", \"for\", \"lunch\", \"yesterday\", \"and\", \"it\", \"was\", \"delicious\"]\n",
    "    return sequence\n",
    "\n",
    "class NGramModel(torch.nn.Module):\n",
    "    def __init__(self, num_words, num_hidden, n=2):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.embedding = torch.nn.Embedding(num_words, num_hidden)\n",
    "        self.linear = torch.nn.Linear(num_hidden * n, num_words)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05949dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, sequence, word_to_idx, learning_rate=1e-2, epochs=100):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "\n",
    "        # Loop through the words in the sequence\n",
    "        for i in range(len(sequence) - model.n):\n",
    "            # Convert the current n-gram and next word to tensors\n",
    "            current_ngram = torch.tensor([word_to_idx[word] for word in sequence[i:i+model.n]], dtype=torch.long)\n",
    "            next_word = torch.tensor([word_to_idx[sequence[i + model.n]]], dtype=torch.long)\n",
    "\n",
    "            # Predict the probability of the next word\n",
    "            logits = model(current_ngram.unsqueeze(0))\n",
    "\n",
    "            # Compute the cross-entropy loss and update the totals\n",
    "            loss = criterion(logits, next_word)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute the accuracy and update the totals\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            total_accuracy += (predicted == next_word).sum().item()\n",
    "\n",
    "            # Backpropagate the loss and update the model parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Compute the average loss and accuracy for the epoch and update the relevant lists\n",
    "        avg_loss = total_loss / (len(sequence) - model.n)\n",
    "        avg_accuracy = total_accuracy / (len(sequence) - model.n)\n",
    "        losses.append(avg_loss)\n",
    "        accuracies.append(avg_accuracy)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: loss = {avg_loss:.4f}, accuracy = {avg_accuracy:.4f}\")\n",
    "\n",
    "    return model, losses, accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb93286e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: loss = 0.0497, accuracy = 1.0000\n",
      "Epoch 20: loss = 0.0147, accuracy = 1.0000\n",
      "Epoch 30: loss = 0.0075, accuracy = 1.0000\n",
      "Epoch 40: loss = 0.0046, accuracy = 1.0000\n",
      "Epoch 50: loss = 0.0031, accuracy = 1.0000\n",
      "Epoch 60: loss = 0.0023, accuracy = 1.0000\n",
      "Epoch 70: loss = 0.0017, accuracy = 1.0000\n",
      "Epoch 80: loss = 0.0014, accuracy = 1.0000\n",
      "Epoch 90: loss = 0.0011, accuracy = 1.0000\n",
      "Epoch 100: loss = 0.0009, accuracy = 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Generate the training data\n",
    "sequence = generate_sequence()\n",
    "# Create a vocabulary of unique words in the sequence\n",
    "vocab = list(set(sequence))\n",
    "\n",
    "# Create a mapping from word to index\n",
    "word_to_idx = {word: index for index, word in enumerate(vocab)}\n",
    "\n",
    "# Create a mapping from index to word\n",
    "index_to_word = {index: word for index, word in enumerate(vocab)}\n",
    "\n",
    "# Define the number of words, the number of hidden units, and the value of n\n",
    "num_words = len(set(sequence))\n",
    "num_hidden = 10\n",
    "n = 3\n",
    "\n",
    "# Initialize the n-gram model\n",
    "model = NGramModel(num_words, num_hidden, n)\n",
    "\n",
    "# Train the model\n",
    "model, losses, accuracies = train_model(model,sequence,word_to_idx,1e-2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0927fafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'was': 0,\n",
       " 'sandwich': 1,\n",
       " 'delicious': 2,\n",
       " 'and': 3,\n",
       " 'yesterday': 4,\n",
       " 'I': 5,\n",
       " 'a': 6,\n",
       " 'lunch': 7,\n",
       " 'it': 8,\n",
       " 'for': 9,\n",
       " 'had': 10}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a952dad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'was',\n",
       " 1: 'sandwich',\n",
       " 2: 'delicious',\n",
       " 3: 'and',\n",
       " 4: 'yesterday',\n",
       " 5: 'I',\n",
       " 6: 'a',\n",
       " 7: 'lunch',\n",
       " 8: 'it',\n",
       " 9: 'for',\n",
       " 10: 'had'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a748694",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_ngram = torch.tensor([word_to_idx['had'],word_to_idx['a'],word_to_idx[\"sandwich\"]], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c7000c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_logits = model(_ngram.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45693815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[torch.max(_logits, dim=1)[1].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309b827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
